{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.5 모델 선택에서 평가 지표 사용하기\n",
    "- [note]: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이진 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from preamble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default scoring: [0.9 0.9 0.9]\n",
      "Explicit accuracy scoring: [0.9 0.9 0.9]\n",
      "AUC scoring: [0.994 0.99  0.996]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "# default scoring for classification is accuracy\n",
    "print(\"Default scoring: {}\".format(\n",
    "    cross_val_score(SVC(), digits.data, digits.target == 9)))\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
    "                                     scoring=\"accuracy\")\n",
    "print(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\n",
    "roc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
    "                           scoring=\"roc_auc\")\n",
    "print(\"AUC scoring: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default scoring: [0.9 0.9 0.9]\n",
      "Explicit accuracy scoring: [0.9 0.9 0.9]\n",
      "\n",
      "ROC_AUC scoring: [0.994 0.99  0.996]\n",
      "Average Precision scoring: [0.96  0.953 0.978]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CodeNEX_J\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\CodeNEX_J\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\CodeNEX_J\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision scoring: [0.81 0.81 0.81]\n",
      "Precision scoring: [0.9 0.9 0.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CodeNEX_J\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\CodeNEX_J\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score scoring: [0.852 0.852 0.852]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CodeNEX_J\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target == 9, random_state=0)\n",
    "\n",
    "# default scoring for classification is accuracy\n",
    "scores = cross_val_score(SVC(), digits.data, digits.target == 9)\n",
    "print(\"Default scoring: {}\".format(scores))\n",
    "\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "scores2 = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"accuracy\")\n",
    "print(\"Explicit accuracy scoring: {}\".format(scores2))\n",
    "\n",
    "print()\n",
    "\n",
    "# 곡선의 면적을 활용한 성능 측정 (Recommended)\n",
    "roc_auc = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"roc_auc\")\n",
    "print(\"ROC_AUC scoring: {}\".format(roc_auc))\n",
    "\n",
    "average_precision = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"average_precision\")\n",
    "print(\"Average Precision scoring: {}\".format(average_precision))\n",
    "\n",
    "print()\n",
    "\n",
    "# 다양한 성능 측정 (Not Recommended)\n",
    "precision = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"precision_weighted\")\n",
    "print(\"Precision scoring: {}\".format(precision))\n",
    "\n",
    "recall = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"recall_weighted\")\n",
    "print(\"Precision scoring: {}\".format(recall))\n",
    "\n",
    "f1_score = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"f1_weighted\")\n",
    "print(\"F1_score scoring: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다중 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicit accuracy scoring: [0.394 0.411 0.46 ]\n",
      "F1_weighted scoring: [0.439 0.463 0.524]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(SVC(), digits.data, digits.target, scoring=\"accuracy\")\n",
    "print(\"Explicit accuracy scoring: {}\".format(scores))\n",
    "\n",
    "f1_weighted = cross_val_score(SVC(), digits.data, digits.target, scoring=\"f1_weighted\")\n",
    "print(\"F1_weighted scoring: {}\".format(f1_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GridSearchCV에 다양한 scoring 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid-Search with accuracy\n",
      "Best parameters: {'gamma': 0.0001}\n",
      "Best cross-validation score (accuracy)): 0.970\n",
      "Test set AUC: 0.992\n",
      "Test set accuracy: 0.973\n",
      "\n",
      "Grid-Search with AUC\n",
      "Best parameters: {'gamma': 0.01}\n",
      "Best cross-validation score (AUC): 0.997\n",
      "Test set AUC: 1.000\n",
      "Test set accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target == 9, random_state=0)\n",
    "\n",
    "# we provide a somewhat bad grid to illustrate the point:\n",
    "param_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# using the default scoring of accuracy:\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Grid-Search with accuracy\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation score (accuracy)): {:.3f}\".format(grid.best_score_))\n",
    "print(\"Test set AUC: {:.3f}\".format(roc_auc_score(y_test, grid.decision_function(X_test))))\n",
    "print(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "\n",
    "# using AUC scoring instead:\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Grid-Search with AUC\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_))\n",
    "print(\"Test set AUC: {:.3f}\".format(roc_auc_score(y_test, grid.decision_function(X_test))))\n",
    "print(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scorers:\n",
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
